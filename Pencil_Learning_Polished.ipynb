{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pencil_Learning_Polished.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPS1TbGIuzNckEsln/pZ9LW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Engineering-Geek/Pencil_Learning_Assessment/blob/master/Pencil_Learning_Polished.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu7ws8WzLTj1",
        "colab_type": "text"
      },
      "source": [
        "# Pencil Learning Machine Learning Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQX6OLE0Nouv",
        "colab_type": "text"
      },
      "source": [
        "## Getting the Universal Sentence Encoder as per directions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8DeYjJdLL-B",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Getting Universal Sentence Encoder from Google\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# cuz tensorflow loves to yell at us, I wanna just shut it up now and only\n",
        "#    tell me when I'm literally making a mistake. I can tolerate warnings for now\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" # @param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMZ7A29SNneS",
        "colab_type": "text"
      },
      "source": [
        "## Creating a custom Neural Network \n",
        "\n",
        "\n",
        "\n",
        "*   Layer 1: String input\n",
        "*   Layer 2: The universal sentence encoder\n",
        "*   Layer 3: Hidden layer for basic processing\n",
        "*   Layer 4: Output layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOLlT5SdMOSg",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "def create_custom_model(n_outputs=1000):\n",
        "  def UniversalEmbedding(x):\n",
        "      return model(tf.squeeze(tf.cast(x, tf.string), axis=1))\n",
        "\n",
        "  embed_size = 512 #@param {type:\"raw\"}\n",
        "\n",
        "  input_text = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"input\")\n",
        "  embedding = tf.keras.layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n",
        "  hidden_layer_1 = tf.keras.layers.Dense(units=1000, activation=\"relu\")(embedding)\n",
        "  dropout1 = tf.keras.layers.Dropout(0.2)(hidden_layer_1)\n",
        "  hidden_layer_2 = tf.keras.layers.Dense(units=1000, activation=\"relu\")(dropout1)\n",
        "  dropout2 = tf.keras.layers.Dropout(0.2)(hidden_layer_2)\n",
        "  output_layer = tf.keras.layers.Dense(units=n_outputs, activation=\"sigmoid\")(dropout2)\n",
        "  \n",
        "\n",
        "  custom_model = tf.keras.Model(inputs=input_text, outputs=output_layer)\n",
        "  return custom_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtJfCrP4OjJf",
        "colab_type": "text"
      },
      "source": [
        "## Using a custom Dataset\n",
        "For this, I'm using [\"Pride and Prejudice\"](https://www.gutenberg.org/files/1342/1342-0.txt) and many other books from that same site. However, any txt file should in theory suffice.\n",
        "\n",
        "I also took a little snippit from [StackOverflow](https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences) to help out a lot. I normally never copy + paste without reading/learning everything about the topic and figuring out my own roundabout way. But this snippit of code literally had everything I wanted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvDG3KuEMraE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Run this as many times as you want to get all your scripts in google colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-HTlXDCMsHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# May or may not be copied from stack overflow...\n",
        "# Credit where credit is due: https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences \n",
        "import re\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    alphabets= \"([A-Za-z])\"\n",
        "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "    websites = \"[.](com|net|org|io|gov)\"\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr2LC4I7Mz3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Clean sentences without commas or any punctuation\n",
        "titles = [\n",
        "          \"war_and_peace.txt\",\n",
        "          \"huckleberry_finn.txt\",\n",
        "          \"adventures_of_tom_sawyer.txt\",\n",
        "          \"pride_and_prejudice.txt\"\n",
        "          ]\n",
        "giant_script = []\n",
        "for title in titles:\n",
        "    with open(title, \"r\") as script:\n",
        "        data = script.read()\n",
        "        data = data.replace(\",\", \"\")\n",
        "        data = data.replace(\"”\", \"\\\"\")\n",
        "        data = data.replace(\"“\", \"\\\"\")\n",
        "        data = split_into_sentences(data)\n",
        "        for index in range(len(data)):\n",
        "            data[index] = \" \".join(data[index].strip().split()).replace(\"_\", \"\")\n",
        "            data[index] = data[index].replace(\"\\\"\", \"\")\n",
        "            data[index] = data[index].replace(\"!\", \"\")\n",
        "            data[index] = data[index].replace(\"?\", \"\")\n",
        "            data[index] = data[index].replace(\"(\", \"\")\n",
        "            data[index] = data[index].replace(\")\", \"\")\n",
        "            data[index] = data[index].replace(\"—\", \" \")\n",
        "    giant_script.extend(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EQ3T5a4Rwl9",
        "colab_type": "text"
      },
      "source": [
        "#### Dictionary\n",
        "I'm creating a dictionary in the coloquial sense. \n",
        "\n",
        "```python\n",
        "# NOT THIS\n",
        "{}\n",
        "```\n",
        "but this: \n",
        "\n",
        "![Actual Dictionary](https://s.yimg.com/uu/api/res/1.2/bhi1gefWkG34xPYwl5GQsQ--~B/aD0yNTY7dz0yNTY7YXBwaWQ9eXRhY2h5b24-/https://www.blogcdn.com/www.tuaw.com/media/2009/09/dictionary-256.png)\n",
        "\n",
        "Within the book, words will be repeated. All unique words in the book will be the vocabulary of this neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnyoEdNeM3jE",
        "colab_type": "code",
        "outputId": "8faf45fb-17b3-49db-a233-dc29ff3ffecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create a dictionary with all the words [alphabetically]\n",
        "import numpy as np\n",
        "\n",
        "all_words = []\n",
        "for sentence in giant_script:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        all_words.append(word)\n",
        "unique_items, counts = np.unique(all_words, return_counts=True)\n",
        "DICTIONARY = unique_items\n",
        "print(\"RATIO OF (ALL WORDS : UNIQUE WORDS) = \" + str(len(all_words) / len(DICTIONARY)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RATIO OF (ALL WORDS : UNIQUE WORDS) = 21.398075043236332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb8vBAktTO5m",
        "colab_type": "text"
      },
      "source": [
        "#### Generating the dataset in general\n",
        "This will split every sentence from the book longer than 2 words into two parts. Then the first part of the sentence becomes the \"input\" and the first word of the second part is the \"answer\".\n",
        "\n",
        "For example: \n",
        "* according to all known laws of aviation there is no way that a bee should be able to fly\n",
        "    * according to all known laws of \n",
        "    * aviation there is no way that a bee should be able to fly\n",
        "* Input: \"according to all known laws of\"\n",
        "* Output: \"aviation\"\n",
        "\n",
        "Note that the output will be turned into a 1 hot vector meant to correspond to our dictionary. Also, because we are splitting the sentence at random points, we can iterate over the whole script many times to artificially increase our dataset.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUm1NMtQM6ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each sentence, split it at a random point and store the stuff to the left in an array, and the single next word to another array\n",
        "from random import randint\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "all_words = []\n",
        "id_matrix = np.identity(len(DICTIONARY) + 1, dtype=np.float32)\n",
        "\n",
        "iterations = 1 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "for _ in range(iterations):\n",
        "    for sentence in giant_script:\n",
        "        words = sentence.lower().split(\" \")\n",
        "        if len(words) > 2:\n",
        "            index = randint(1, len(words) - 2)\n",
        "            all_words.append(words)\n",
        "            word = words[index]\n",
        "            dictionary_index = np.where(DICTIONARY==word)\n",
        "            if len(dictionary_index[0]) != 0:\n",
        "                one_hot_array = id_matrix[dictionary_index][0]\n",
        "                X.append(' '.join(word for word in words[:index]))\n",
        "                y.append(one_hot_array)\n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXDG7-s4WF9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split as split\n",
        "# _tv   --> Train + Validation (keras.fit has a validation split option)\n",
        "# _test --> duh\n",
        "X_tv, X_test, y_tv, y_test = split(X, y, test_size=0.33, random_state=420)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4onznnwzVUfC",
        "colab_type": "text"
      },
      "source": [
        "## Customizing our Neural Network from earlier with specifications\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtYwIL_Cq8vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "# Because we have an incredibly imbalanced dataset where words like \"the\", \"or\"\n",
        "#   and such are disproportionately shown, optimizing by f1 is the best here\n",
        "def f1(y_true, y_pred): # taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphy3tFjM7un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_model = create_custom_model(n_outputs=len(y[0]))\n",
        "custom_model.compile(\n",
        "    optimizer=\"Adam\", \n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[f1]\n",
        "    )\n",
        "\n",
        "epochs = 100 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "history = custom_model.fit(\n",
        "    X_tv, \n",
        "    y_tv, \n",
        "    validation_split=0.3, \n",
        "    batch_size=128, \n",
        "    epochs=epochs\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHMsqRJzX_YD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = custom_model.evaluate(X_test, y_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fkF5Jn6YoGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Play with it just a bit and check it out!\n",
        "sentence = \"Hello, how are\" #@param {type: \"raw\"}\n",
        "result_vector = custom_model.predict(np.asarray([sentence]))\n",
        "dictionary_index = tf.keras.backend.argmax(result_vector)\n",
        "print(DICTIONARY[dictionary_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRKfGUPdGBH",
        "colab_type": "text"
      },
      "source": [
        "## CONCLUSION\n",
        "Unsurprisingly, common words in the English Language like \"the\" or \"to\" are suggested FAR more frequently than they should. The neural network seems to be defaulting to those in any uncertainty because it's likely to be right.\n",
        "\n",
        "The optimal number of epochs before overfitting seems to be 5-6 epochs. The val_f1 score seems to be peaking at 0.12 with the books I'm using, but with every new \"book\" appended onto this, it goes up a little bit. Given a big enough library, I believe we could get this neural network to function at a val_f1 of about 0.2-0.5 (can't tell without actually doing it). It likely won't go up from there because the neural network as of now is unable to categorize certain words like \"nouns\", \"prepositions\", \"verbs\" to optimize it's prediction based off of it. The current neural network is only a proof of concept that the neural network can learn SOMETHING with a standard training regime. There are NUMEROUS ways to improve this code. If I was given more time to pursue this task, here's how I would go about it.\n",
        "\n",
        "First, here is the current code I have:\n",
        "\n",
        "![Current Model](https://i.imgur.com/S4xQOd8.png)\n",
        "\n",
        "As described earlier, there's just simply too many words in which only a handful are far more common than the rest and even with using cross entropy and f1 scoring, it's hard to get good results. As such, we could make a better network by creating a new neural network that can tell us what kind of word should come next. For example\n",
        "\n",
        "- according to all known laws of\n",
        "- aviation [noun]\n",
        "\n",
        "After figuring out what kind of word is next, we could direct it to another custom neural network to select the best noun, verb, etc given the universal language encoding. Just as shown below\n",
        "\n",
        "![Better Model](https://imgur.com/d83ffPJ.png)\n",
        "\n",
        "Unfortunately due to time constraints, I wasn't able to impliment this methodology. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3WtkJm8ev_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}